{
    "architecture": "XiaoanModelForCausalLM",
    "dtype": "float16",
    "hidden_size": 4096,
    "vocab_size": 151936,
    "num_hidden_layers": 1,
    "num_attention_heads": 1
}
